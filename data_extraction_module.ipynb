{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables used for extracting the player attributes\n",
    "\n",
    "st, mid, df, gk = 'Striker', 'Midfielder', 'Defender', 'GoalKeeper'\n",
    "player_category_map = {'LW':st, 'ST':st, 'RW':st, 'LF':st, 'CF':st, 'RF':st,\n",
    "                      'CAM':mid, 'LM':mid, 'CM':mid, 'RM':mid, 'CDM':mid,\n",
    "                      'LWB':df, 'LB':df, 'CB':df, 'RB':df, 'RWB':df,\n",
    "                      'GK': gk}\n",
    "content_aux_list = ['meta', 'column col-4 text-center']\n",
    "\n",
    "attr_list = ['Crossing', 'Finishing', 'Heading Accuracy', 'Short Passing', 'Volleys', 'Dribbling','Curve',\n",
    "             'FK Accuracy', 'Long Passing', 'Ball Control', 'Acceleration', 'Sprint Speed', 'Agility', \n",
    "             'Reactions', 'Balance', 'Shot Power', 'Jumping', 'Stamina', 'Strength', 'Long Shots', \n",
    "             'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure', 'Marking',\n",
    "             'Standing Tackle', 'Sliding Tackle', 'GK Diving', 'GK Handling', 'GK Kicking',\n",
    "             'GK Positioning', 'GK Reflexes']\n",
    "aux_attr_list = ['Player Category', 'Age', 'Height', 'Weight', 'Overall Rating', 'Value', 'Wage']\n",
    "\n",
    "attr_len = len(attr_list)\n",
    "aux_attr_len = len(aux_attr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_of_page(player_url):\n",
    "    ''' \n",
    "    Returns all the words of each of the webpage as a list given the page url\n",
    "    \n",
    "    Arguments:\n",
    "    player_url (str) : The webpage url of the player\n",
    "    \n",
    "    Returns:\n",
    "    lines (list) : The list of words contained in the webpage\n",
    "    '''\n",
    "    \n",
    "    assert isinstance(player_url,  str), 'Player url is not a string'\n",
    "    \n",
    "    page = requests.get(player_url)\n",
    "    soup = bs(page.content, 'lxml')\n",
    "    content = soup.find_all('ul', {'class': 'pl'})\n",
    "\n",
    "    lines = []\n",
    "    for c in content:\n",
    "        line = c.text.strip().split()\n",
    "        new_line = []\n",
    "        for word in line:\n",
    "            if word[0].isdigit(): \n",
    "                new_line.append(word)\n",
    "                continue\n",
    "            new_word = ''\n",
    "            number = ''\n",
    "            for letter in word:\n",
    "                if letter.isdigit() or letter == '+' or letter == '-':   number += letter\n",
    "                else:                                                    new_word += letter\n",
    "            if len(new_word) != 0:    new_line.append(new_word)\n",
    "            if len(number) != 0:      new_line.append(number)\n",
    "        lines.append(new_line)\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_player_attributes(player_name, player_url):\n",
    "    '''\n",
    "    Extracts and returns the attibute ratings of a player given the player name and the player url\n",
    "    \n",
    "    Arguments:\n",
    "    player_name (str) : The name of the player\n",
    "    player_url  (str) : The website url of the player\n",
    "    \n",
    "    Returns:\n",
    "    player_attributes (dict) : The dictionary containing the player attribute ratings\n",
    "    '''\n",
    "    \n",
    "    assert isinstance(player_name, str), 'Player name is not a string'\n",
    "    assert isinstance(player_url,  str), 'Player url  is not a string'\n",
    "\n",
    "    player_attributes = {}\n",
    "    \n",
    "    # get all the words in the player's webpage to extract the attribute ratings \n",
    "    lines = words_of_page(player_url)\n",
    "    \n",
    "    # fetching the attribute ratings by matching the words with list of attributes given by 'attr_list'\n",
    "    attr_ratings = []\n",
    "    attr_count = 0\n",
    "    for line in lines:\n",
    "        for i,word in enumerate(line):\n",
    "            if word[0].isdigit():\n",
    "                new_word = ''\n",
    "                for next_word in line[i+1:]:\n",
    "                    if next_word[0].isdigit(): \n",
    "                        break\n",
    "                    else:\n",
    "                        if len(new_word) == 0:    new_word += next_word\n",
    "                        else :                    new_word += ' ' + next_word\n",
    "                if new_word == attr_list[attr_count]:\n",
    "                    attr_ratings.append(word)\n",
    "                    if attr_count < attr_len-1:   attr_count += 1\n",
    "    for i, rating in enumerate(attr_ratings):\n",
    "        if len(rating.split('+')) != 1:   attr_ratings[i] = rating.split('+')[0]\n",
    "        if len(rating.split('-')) != 1:   attr_ratings[i] = rating.split('-')[0]\n",
    "    if attr_count != attr_len-1:\n",
    "        print(player_name, '- player not included')   # due to missed attributes, if any\n",
    "        return {}\n",
    "\n",
    "    # fetching the auxiliary attribute data by matching the words with the 'aux_attr_list'\n",
    "    aux_attr_data = []\n",
    "    for item in content_aux_list:\n",
    "        page = requests.get(player_url)\n",
    "        soup = bs(page.content, 'lxml')\n",
    "        content_aux = soup.find_all('div', {'class': item})\n",
    "        for c in content_aux:\n",
    "            line = c.text.strip().split()\n",
    "            for i,word in enumerate(line):\n",
    "                word_prev = line[i-1]\n",
    "                if word[0].isdigit() and word_prev[-3:] == aux_attr_list[1]:\n",
    "                    if len(word_prev[:-3]) < 2:   \n",
    "                        aux_attr_data.append(player_category_map[line[i-2]])\n",
    "                    else:\n",
    "                        aux_attr_data.append(player_category_map[word_prev[:-3]])\n",
    "                    aux_attr_data.append(word)\n",
    "                    height, weight = line[-2], line[-1][:-3]\n",
    "                    aux_attr_data.append(height)\n",
    "                    aux_attr_data.append(weight)\n",
    "                if word_prev + ' ' + word == aux_attr_list[4] or word == aux_attr_list[5] or word == aux_attr_list[6]:\n",
    "                    word_next = line[i+1]\n",
    "                    aux_attr_data.append(word_next)\n",
    "    if len(aux_attr_data) != aux_attr_len:\n",
    "        print(player_name, '- player not included')   # due to missed auxiliary attributes, if any\n",
    "        return {}\n",
    "\n",
    "    # storing all the required player attributes as a dictionary\n",
    "    player_attributes[player_name] = aux_attr_data + attr_ratings\n",
    "\n",
    "    return player_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trim_save_dataframe(attributes_of_all_the_players):\n",
    "    '''\n",
    "    Creates the player attributes dataframe from the given player attributes dictionary, trims it and saves it as '.csv' file\n",
    "    \n",
    "    Arguments:\n",
    "    attributes_of_all_the_players (dict) : Attributes of all the players\n",
    "    '''\n",
    "    \n",
    "    assert isinstance(attributes_of_all_the_players, dict), \"Attributes of all the players is not a dictionary\"\n",
    "    \n",
    "    # creating the dataframe\n",
    "    player_attr_dataframe = pd.DataFrame(columns = ['Player Name'] + aux_attr_list + attr_list)\n",
    "    for name, ratings in attributes_of_all_the_players.items():\n",
    "        player_data = [name]\n",
    "        for rating in ratings:   player_data.append(rating)\n",
    "        player_attr_dataframe = player_attr_dataframe.append(pd.Series(player_data,\n",
    "                                index = ['Player Name'] + aux_attr_list + attr_list), ignore_index=True)\n",
    "\n",
    "    # trimming the dataframe\n",
    "    players_per_category = {st:0, mid:0, df:0, gk:0}\n",
    "    desired_players_per_category = {st:2000, mid:4000, df:3000, gk:1000}\n",
    "    for row in player_attr_dataframe.iterrows():\n",
    "        category = row[1][1]\n",
    "        if players_per_category[category] != desired_players_per_category[category]:\n",
    "            players_per_category[category] += 1\n",
    "        else:\n",
    "            player_attr_dataframe = player_attr_dataframe.drop(row[0])\n",
    "    player_attr_dataframe = player_attr_dataframe.reset_index(drop=True)\n",
    "    print('Number of players in each category:\\n', players_per_category)\n",
    "\n",
    "    # saving the dataframe\n",
    "    player_attr_dataframe.to_csv('player_attributes.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
